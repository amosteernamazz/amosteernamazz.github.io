---
layout: article
title: GEMM优化
key: 100026
tags: GEMM GPU性能优化
category: blog
date: 2023-02-08 00:00:00 +08:00
mermaid: true
---









# 各种实现GEMM的方法

  ```c++
    #define CEIL_DIV(m,n) ( (m) + (n) - 1 ) / (n)
    // 函数调用
    void test_mysgemm_v1(INT M, INT N, INT K, FLOAT alpha, FLOAT *A, FLOAT *B, FLOAT beta, FLOAT *C){
        cudaDeviceSynchronize();
        dim3 blockDim(32,32);
        // 由block大小32，确定grid大小，利用M+32-1/32并舍弃小数点得到
        dim3 gridDim(CEIL_DIV(M,32),CEIL_DIV(N,32));
        // 其中A(M*K)、B(K*N)、C(M*N)各个矩阵都为列存储
        mysgemm_v1<<<gridDim, blockDim>>>(M,N,K,alpha,A,B,beta,C);
        cudaDeviceSynchronize();
    }
  ```

<!--more-->

## Version 1: 只使用并发处理

  ```c++

  #include<stdio.h>
  #include<stdlib.h>
  // A、B、C为列主序矩阵, #define A(i,j) A[(i)+(j)*lda]管理A(i,j)访问与列主序存储的映射
  #define A(i,j) A[(i) + (j)*lda]
  #define B(i,j) B[(i) + (j)*ldb]
  #define C(i,j) C[(i) + (j)*ldc]
  // naive version
  // 限制一个block的threads个数为1024
  __global__  __launch_bounds__(1024)
  // __launch_bounds__(1024) 限制每个SM的最多线程数
  void mysgemm_v1(int M, int N, int K, float alpha, float* A, float* B, float beta, float* C){
      //在一个block内的处理
      int lda = M, ldb = K, ldc = M;
      int tx = threadIdx.x, ty = threadIdx.y;
      int bx = blockIdx.x, by = blockIdx.y;
      // 确定block的位置
      A = &A((bx<<5),0);
      B = &B(0,(by<<5));
      C = &C((bx<<5),(by<<5));
      float tmp=0.;
      for (int k_count = 0; k_count<K; k_count++){
          // 对列主序来说，读A更快，当tx变化，对应A的内存可以合并访问，而B的内存回触发多次transaction
          tmp += A(tx, k_count) * B(k_count, ty);
      }
      C(tx,ty) = alpha * tmp + beta*C(tx,ty);
  }
  ```




## Version 2: 加入shared memory，但没有优化
  ```c++
  #include<stdio.h>
  #include<stdlib.h>
  #define A(i,j) A[(i) + (j)*lda]
  #define B(i,j) B[(i) + (j)*ldb]
  #define C(i,j) C[(i) + (j)*ldc]
  #define sa(i,j) sa[((i)<<5) + (j)]
  #define sb(i,j) sb[((i)<<5) + (j)]
  #define MS 32
  #define NS 32
  #define KS 32
  // cache blocking version, without register-level data re-use
  __global__  __launch_bounds__(1024)
  void mysgemm_v2(int M, int N, int K, float alpha, float* A, float* B, float beta, float* C){
      int lda = M, ldb = K, ldc = M;
      int tx = threadIdx.x, ty = threadIdx.y;
      int bx = blockIdx.x, by = blockIdx.y;
      A = &A((bx<<5),0);
      B = &B(0,(by<<5));
      C = &C((bx<<5),(by<<5));
      // 加入shared memory，大小为32*32
      __shared__ float sa[MS*KS];
      __shared__ float sb[KS*NS];
      float tmp=0.;
      for (int k_count = 0; k_count<K; k_count+=KS){
          // 为了防止bank conflict将，sb设置为转置
          sa(tx,ty)=A(tx,ty);
          sb(ty,tx)=B(tx,ty);
          A+=(lda<<5);B+=32;
          __syncthreads();
          for (int inner_k_count=0;inner_k_count<KS;inner_k_count++){
              // 看是否出现shared memory的bank conflict问题，变化的是tx当tx变化，对应到sa空间的变化为1024大小的变化，会产生bank conflicts，优化是是将shared memory存储转化为列主序，见kenrel4
              tmp += sa(tx,inner_k_count) * sb(ty,inner_k_count);
          }
          __syncthreads();
      }
      C(tx,ty) = alpha * tmp + beta*C(tx,ty);
  }
  ```
## Version 3: threadId设为register存储



  ```c++
  void test_mysgemm_v3(INT M, INT N, INT K, FLOAT alpha, FLOAT *A, FLOAT *B, FLOAT beta, FLOAT *C){
      cudaDeviceSynchronize();
      dim3 blockDim(1024);
      dim3 gridDim(CEIL_DIV(M,32),CEIL_DIV(N,32));
      mysgemm_v3<<<gridDim, blockDim>>>(M,N,K,alpha,A,B,beta,C);
      cudaDeviceSynchronize();
  }
  ```

  ```c++
  #include<stdio.h>
  #include<stdlib.h>
  #define A(i,j) A[(i) + (j)*lda]
  #define B(i,j) B[(i) + (j)*ldb]
  #define C(i,j) C[(i) + (j)*ldc]
  #define sa(i,j) sa[((i)<<5) + (j)]
  #define sb(i,j) sb[((i)<<5) + (j)]
  #define MS 32
  #define NS 32
  #define KS 32
  // cache blocking version, without register-level data re-use
  // save one living register ty.
  __global__  __launch_bounds__(1024)
  void mysgemm_v3(int M, int N, int K, float alpha, float* A, float* B, float beta, float* C){

    // 为local存储
      int lda = M, ldb = K, ldc = M;
      int tx = threadIdx.x;
      int bx = blockIdx.x, by = blockIdx.y;
      // 利用register进行优化，row和col分别位对应shared memory对应的位置，其中在<<<grid, block>>>中，block直接设置为1维度，row和col用于存储当前行和当前列（thread层面）。tx的前31表示对应的row，tx/32表示对应的col

      // 为register存储
      int row = tx&31, col = tx>>5;
      // 定位当前block的thread
      A = &A((bx<<5),0);
      B = &B(0,(by<<5));
      C = &C((bx<<5),(by<<5));
      __shared__ float sa[MS*KS];
      __shared__ float sb[KS*NS];
      // 为local存储
      float tmp=0.;
      for (int k_count = 0; k_count<K; k_count+=KS){
          sa(row,col)=A(row,col);
          sb(col,row)=B(row,col);
          A+=(lda<<5);B+=32;
          __syncthreads();
          for (int inner_k_count=0;inner_k_count<KS;inner_k_count++){

              tmp += sa(row,inner_k_count) * sb(col,inner_k_count);
          }
          __syncthreads();
      }
      C(row,col) = alpha * tmp + beta*C(row,col);
  }
  ```
## Version 4: 去除shared memory的bank conflict
  ```c++
  #include<stdio.h>
  #include<stdlib.h>
  #define A(i,j) A[(i) + (j)*lda]
  #define B(i,j) B[(i) + (j)*ldb]
  #define C(i,j) C[(i) + (j)*ldc]
  #define sa4(i,j) sa4[((j)<<5) + (i)]
  #define sb4(i,j) sb4[((j)<<5) + (i)]
  #define MS 32
  #define NS 32
  #define KS 32
  // cache blocking version, without register-level data re-use
  // with memory coelascing on shared memory
  __global__  __launch_bounds__(1024)
  void mysgemm_v4(int M, int N, int K, float alpha, float* A, float* B, float beta, float* C){
      int lda = M, ldb = K, ldc = M;
      int tx = threadIdx.x;
      int bx = blockIdx.x, by = blockIdx.y;
      int row = tx&31, col = tx>>5;
      A = &A((bx<<5),0);
      B = &B(0,(by<<5));
      C = &C((bx<<5),(by<<5));
      __shared__ float sa4[MS*KS];
      __shared__ float sb4[KS*NS];
      float tmp=0.;
      for (int k_count = 0; k_count<K; k_count+=KS){
          sa4(row,col)=A(row,col);
          sb4(col,row)=B(row,col);
          A+=(lda<<5);B+=32;
          __syncthreads();
          // 将shared memory转换为列存储，解决了shared memory中存在的bank conflict问题，当row变化时候，对应存储在shared memory为顺序结构
          for (int inner_k_count=0;inner_k_count<KS;inner_k_count++){
              tmp += sa4(row,inner_k_count) * sb4(col,inner_k_count);
          }
          __syncthreads();
      }
      C(row,col) = alpha * tmp + beta*C(row,col);
  }
  ```
## Version 5: 设立 4*1 micro-kernel
  ```c++
  void test_mysgemm_v5(INT M, INT N, INT K, FLOAT alpha, FLOAT *A, FLOAT *B, FLOAT beta, FLOAT *C){
      cudaDeviceSynchronize();
      dim3 blockDim(256);
      dim3 gridDim(CEIL_DIV(M,32),CEIL_DIV(N,32));
      mysgemm_v5<<<gridDim, blockDim>>>(M,N,K,alpha,A,B,beta,C);
      cudaDeviceSynchronize();
  }
  ```

  ```c++
  #include<stdio.h>
  #include<stdlib.h>
  // A、B、C为列主序矩阵
  #define A(i,j) A[(i) + (j)*lda]
  #define B(i,j) B[(i) + (j)*ldb]
  #define C(i,j) C[(i) + (j)*ldc]
  // shared memory也为列主序矩阵
  #define sa5(i,j) sa5[((j)<<5) + (i)]
  #define sb5(i,j) sb5[((j)<<5) + (i)]
  #define MS 32
  #define NS 32
  #define KS 32
  // cache blocking version, without register-level data re-use
  // with memory coelascing on shared memory
  // more workloads per thread. 4x1 micro kernel.
  // kenrel前面数目为1024，因为SM对threads的数目有限制，为了提高SM的block数目，一个thread处理4个数据，减少每个block的threads需求数目，提高SM的blocks数目
  __global__  __launch_bounds__(256)
  void mysgemm_v5(int M, int N, int K, float alpha, float* A, float* B, float beta, float* C){
      int lda = M, ldb = K, ldc = M;
      int tx = threadIdx.x;
      int bx = blockIdx.x, by = blockIdx.y;
      // 确定当前行和当前列，其中shared memory为32*32，利用下述分析，其中每个kernel负责4*1的矩阵计算，需要4个row、1个col，利用32/4 = 8
      // 例如对tx= 0000和tx= 0100应得到相同的row和col
      // tx & 0111得到对应的index，0000和0100，利用对应index相同得到<<2
      // 0000 & 0111 = 000 <<2 = 0 row2 = 1 row3 = 2 row4 = 3
      // 0001 & 0111 = 001 <<2 = 100 4 row2 = 101 5 row3 = 110 6 tow4 = 111 7
      int row1 = (tx&7)<<2, row2 = row1+1, row3 = row1+2, row4 = row1+3, col = tx>>3;
      // 找到每个block的对应起始位置(C(i,j)表示A(i:)与B(:j)乘积的和)
      A = &A((bx<<5),0);
      B = &B(0,(by<<5));
      C = &C((bx<<5),(by<<5));
      // shared memory保存每个block的数据
      __shared__ float sa5[MS*KS];
      __shared__ float sb5[KS*NS];
      // 每个thread负责4个的计算
      float Cres[4] = {0., 0., 0., 0.};
      float b00;
      // 将shared memory的数据保存到
      for (int k_count = 0; k_count<K; k_count+=KS){
          sa5(row1,col)=A(row1,col);
          sa5(row2,col)=A(row2,col);
          sa5(row3,col)=A(row3,col);
          sa5(row4,col)=A(row4,col);
          sb5(col,row1)=B(row1,col);
          sb5(col,row2)=B(row2,col);
          sb5(col,row3)=B(row3,col);
          sb5(col,row4)=B(row4,col);
          A+=(lda<<5);B+=32;
          __syncthreads();
          #pragma unroll
          // 对每个block中进行计算
          for (int inner_k_count=0;inner_k_count<KS;inner_k_count++){
              b00 = sb5(col,inner_k_count);
              Cres[0] += sa5(row1,inner_k_count) * b00;
              Cres[1] += sa5(row2,inner_k_count) * b00;
              Cres[2] += sa5(row3,inner_k_count) * b00;
              Cres[3] += sa5(row4,inner_k_count) * b00;
          }
          __syncthreads();
      }
      C(row1,col) = alpha * Cres[0] + beta*C(row1,col);
      C(row2,col) = alpha * Cres[1] + beta*C(row2,col);
      C(row3,col) = alpha * Cres[2] + beta*C(row3,col);
      C(row4,col) = alpha * Cres[3] + beta*C(row4,col);
  }
  ```
## Version 6: 采用float4数据格式用于数据传输

```c++
#include<stdio.h>
#include<stdlib.h>
#define A(i,j) A[(i) + (j)*lda]
#define B(i,j) B[(i) + (j)*ldb]
#define C(i,j) C[(i) + (j)*ldc]
#define sa6(i,j) sa6[((j)<<5) + (i)]
#define sb6(i,j) sb6[((j)<<5) + (i)]
#define MS 32
#define NS 32
#define KS 32
// cache blocking version, without register-level data re-used
// with memory coelascing on shared memory
// more workloads per thread. 4x1 micro kernel.
// adopt vetorized load/store
__global__  __launch_bounds__(256)
void mysgemm_v6(int M, int N, int K, float alpha, float* A, float* B, float beta, float* C){
    int lda = M, ldb = K, ldc = M;
    int tx = threadIdx.x;
    int bx = blockIdx.x, by = blockIdx.y;
    int row1 = (tx&7)<<2, row2 = row1+1, row3 = row1+2, row4 = row1+3, col = tx>>3;
    A = &A((bx<<5),0);
    B = &B(0,(by<<5));
    C = &C((bx<<5),(by<<5));
    __shared__ float sa6[MS*KS];
    __shared__ float sb6[KS*NS];
    // 利用支持128 transaction(即4 floats)的硬件特点使用float4来做数据的传输
    float4 Av, Bv, Cv, Cres;
    Cres.x = 0., Cres.y = 0., Cres.z = 0., Cres.w = 0.;
    float b00;
    for (int k_count = 0; k_count<K; k_count+=KS){
        // 从global memory到shared memory传输的单位为float4
        Av = *((float4 *)(&A(row1,col)));
        Bv = *((float4 *)(&B(row1,col)));
        ((float4 *)sa6)[tx] = Av;
        sb6(col,row1)=Bv.x;
        sb6(col,row2)=Bv.y;
        sb6(col,row3)=Bv.z;
        sb6(col,row4)=Bv.w;
        A+=(lda<<5);B+=32;
        __syncthreads();
        #pragma unroll
        for (int inner_k_count=0;inner_k_count<KS;inner_k_count++){
            b00 = sb6(col, inner_k_count);
            Cres.x += sa6(row1,inner_k_count) * b00;
            Cres.y += sa6(row2,inner_k_count) * b00;
            Cres.z += sa6(row3,inner_k_count) * b00;
            Cres.w += sa6(row4,inner_k_count) * b00;
        }
        __syncthreads();
    }
    // 从global memory读使用128位读的transaction，写因为硬件限制只能使用32位，在此处使用float4同样进行封装
    Cv = *((float4 *)(&C(row1,col)));
    Cres.x = alpha * Cres.x + beta * Cv.x;
    Cres.y = alpha * Cres.y + beta * Cv.y;
    Cres.z = alpha * Cres.z + beta * Cv.z;
    Cres.w = alpha * Cres.w + beta * Cv.w;
    *(float4 *)(&(C(row1,col))) = Cres;
}
```

## Version 7: 设立 4*4 micro-kernel
  ```c++
    void test_mysgemm_v7(INT M, INT N, INT K, FLOAT alpha, FLOAT *A, FLOAT *B, FLOAT beta, FLOAT *C){
      cudaDeviceSynchronize();
      dim3 blockDim(256);
      // 使用64*64的阵，其中每个block计算4*4阵，对应有256threads
      dim3 gridDim(CEIL_DIV(M,64),CEIL_DIV(N,64));
      mysgemm_v7<<<gridDim, blockDim>>>(M,N,K,alpha,A,B,beta,C);
      cudaDeviceSynchronize();
  }
  ```
  ```c++
  #include<stdio.h>
  #include<stdlib.h>
  #define A(i,j) A[(i) + (j)*lda]
  #define B(i,j) B[(i) + (j)*ldb]
  #define C(i,j) C[(i) + (j)*ldc]
  #define sa7(i,j) sa7[((j)<<6) + (i)]
  #define sb7(i,j) sb7[((j)<<6) + (i)]
  #define MS_7 64
  #define NS_7 64
  #define KS_7 16
  //v1 += v2 * s3, vector scaling
  #define vscal(v1, v2, s3)\
    v1.x+=v2.x*s3;\
    v1.y+=v2.y*s3;\
    v1.z+=v2.z*s3;\
    v1.w+=v2.w*s3;
  //v1 = alpha * v2 + beta * v3, simd fma
  #define simd_axpby(v1, alpha, v2, beta, v3)\
    v1.x=alpha*v2.x+beta*v3.x;\
    v1.y=alpha*v2.y+beta*v3.y;\
    v1.z=alpha*v2.z+beta*v3.z;\
    v1.w=alpha*v2.w+beta*v3.w;
  #define vload(v1,addr)\
    v1 = *((float4 *)(addr));
  #define vstore(addr,v1)\
    *((float4 *)(addr)) = v1;
  // cache blocking version, without register-level data re-use
  // with memory coelascing on shared memory
  // more workloads per thread. 4x4 micro kernel.
  // adopt vetorized load/store
  // 使用64*64的阵，其中每个block计算4*4阵，对应有256threads.
  __global__  __launch_bounds__(256)
  void mysgemm_v7(int M, int N, int K, float alpha, float* A, float* B, float beta, float* C){
    int lda = M, ldb = K, ldc = M;
    int tx = threadIdx.x;
    int bx = blockIdx.x, by = blockIdx.y;
    // 参数如何思考：首先data阵的大小为64*64，每个thread处理4*4，一共256个threads，其中A矩阵为64*16 B为16*64，对应的到A的threads阵为16*x、B为4*y，因此cola为>>4、row为tx&15，colb为>>2、row为tx&3，由每个threads处理4个row得到rowa和rowb<<2
    int row_a = (tx&15)<<2, col_a = tx>>4;
    // 对列的处理以4个为基本单位，如对64*64的阵来说，每个thread处理的分为每4个一块的col
    int row_b = (tx&3)<<2, col_b = tx>>2;
    // 得到所求矩阵对应thread内部列
    int col_c = col_a<<2;
    int lda16 = lda<<4;
    A = &A((bx<<6),0);                         
    B = &B(0,(by<<6));
    C = &C((bx<<6),(by<<6));//the TB size is 64.
    __shared__ float sa7[1024];
    __shared__ float sb7[1024];
    float4 Av, Bv, Cv[4], Cres[4];
    memset(Cres, 0, sizeof(Cres));
    // 保存shared memory
    for (int k_count = 0; k_count<K; k_count+=KS_7){
        vload(Av, &A(row_a,col_a))
        vload(Bv, &B(row_b,col_b))
        ((float4 *)sa7)[tx] = Av;
        sb7(col_b,row_b)=Bv.x;
        sb7(col_b,row_b+1)=Bv.y;
        sb7(col_b,row_b+2)=Bv.z;
        sb7(col_b,row_b+3)=Bv.w;
        A+=lda16;B+=16;
        __syncthreads();
        #pragma unroll
        // 计算矩阵乘法
        for (int inner_k_count=0;inner_k_count<KS_7;inner_k_count++){
            vload(Av, &sa7(row_a,inner_k_count))
            vload(Bv, &sb7(col_c,inner_k_count))
            vscal(Cres[0], Av, Bv.x)
            vscal(Cres[1], Av, Bv.y)
            vscal(Cres[2], Av, Bv.z)
            vscal(Cres[3], Av, Bv.w)
        }
        __syncthreads();
    }
    vload(Cv[0], &C(row_a,col_c))
    vload(Cv[1], &C(row_a,col_c+1))
    vload(Cv[2], &C(row_a,col_c+2))
    vload(Cv[3], &C(row_a,col_c+3))
    simd_axpby(Cres[0],alpha,Cres[0],beta,Cv[0])
    simd_axpby(Cres[1],alpha,Cres[1],beta,Cv[1])
    simd_axpby(Cres[2],alpha,Cres[2],beta,Cv[2])
    simd_axpby(Cres[3],alpha,Cres[3],beta,Cv[3])

    // 保存
    vstore(&C(row_a,col_c), Cres[0])
    vstore(&C(row_a,col_c+1), Cres[1])
    vstore(&C(row_a,col_c+2), Cres[2])
    vstore(&C(row_a,col_c+3), Cres[3])
  }
  ```

## version 8：设立8*8 micro-kernel
  ```c++
  void test_mysgemm_v8(INT M, INT N, INT K, FLOAT alpha, FLOAT *A, FLOAT *B, FLOAT beta, FLOAT *C){
    cudaDeviceSynchronize();
    dim3 blockDim(256);
    // 使用128*128的阵，其中每个block计算8*8阵，对应有256threads
    dim3 gridDim(CEIL_DIV(M,128),CEIL_DIV(N,128));
    mysgemm_v8<<<gridDim, blockDim>>>(M,N,K,alpha,A,B,beta,C);
    cudaDeviceSynchronize();
  }
  ```
